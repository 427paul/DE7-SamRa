from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import shutil
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint, ChatHuggingFace
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# --- í™˜ê²½ ì„¤ì • ë° ìµœì í™” ---
# ëª¨ë¸ì„ ë§¤ë²ˆ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ë¡œì»¬ ìºì‹œ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤. 
# (Airflow ì›Œì»¤ì˜ ë³¼ë¥¨ ë§ˆìš´íŠ¸ ê²½ë¡œë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
os.environ["HUGGINGFACE_HUB_CACHE"] = "/opt/airflow/cache/huggingface"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_huggingface_token"

DB_PATH = "/opt/airflow/data/chroma_db"
BASE_URL = "https://www.mois.go.kr"
LIST_URL = "https://www.mois.go.kr/frt/bbs/type001/commonSelectBoardList.do?bbsId=BBSMSTR_000000000336"

SLACK_WEBHOOK_URL = ("slack_webhook_url")

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# --- 1. ë³´ê³ ì„œ í¬ë¡¤ë§ ë° PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
def extract_report_text(**kwargs):
    import PyPDF2
    print("ðŸš€ ìµœì‹  ë³´ê³ ì„œ í¬ë¡¤ë§ ì‹œìž‘...")
    
    response = requests.get(LIST_URL)
    soup = BeautifulSoup(response.text, "html.parser")
    
    first_row = soup.select_one("table tbody tr:nth-of-type(1) a")
    if not first_row:
        raise Exception("ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    detail_url = urljoin(BASE_URL, first_row["href"])
    detail_resp = requests.get(detail_url)
    detail_soup = BeautifulSoup(detail_resp.text, 'html.parser')
    
    file_list_div = detail_soup.find('div', class_='fileList')
    download_link_tag = file_list_div.find('a')
    full_download_url = urljoin(BASE_URL, download_link_tag['href'])
    modified_url = full_download_url.replace("fileSn=0", "fileSn=1")
    
    # [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ í™•ì¸
    pdf_path = "/tmp/today_report.pdf"
    
    # [ìˆ˜ì •] ë‹¤ìš´ë¡œë“œ ì‹¤í–‰ (ë” ì•ˆì „í•œ ë°©ì‹)
    print(f"ðŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {modified_url}")
    with requests.get(modified_url, stream=True) as r:
        r.raise_for_status() # HTTP ì—ëŸ¬ ì²´í¬
        with open(pdf_path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
    
    # [ìˆ˜ì •] íŒŒì¼ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        
    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. í¬ê¸°: {os.path.getsize(pdf_path)} bytes")

    text = ""
    try:
        with open(pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text()
        print("âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
    finally:
        # ì—ëŸ¬ê°€ ë‚˜ë”ë¼ë„ ìž„ì‹œ íŒŒì¼ì€ ì‚­ì œ ì‹œë„
        if os.path.exists(pdf_path):
            os.remove(pdf_path)
            
    kwargs['ti'].xcom_push(key='report_text', value=text)

# --- 2. Vector DB ìƒì„± (ì‹ ì„ ë„ ìœ ì§€) ---
def build_vector_store(**kwargs):
    text = kwargs['ti'].xcom_pull(key='report_text', task_ids='extract_report_task')
    
    # [ì¤‘ìš”] ê¸°ì¡´ DB ì‚­ì œ (ì˜¤ëŠ˜ ë°ì´í„°ë§Œ ìœ ì§€í•˜ê¸° ìœ„í•¨)
    if os.path.exists(DB_PATH):
        shutil.rmtree(DB_PATH)
        print(f"ðŸ§¹ ê¸°ì¡´ DB ì‚­ì œ ì™„ë£Œ: {DB_PATH}")
    
    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100, separator='\n')
    split_texts = text_splitter.split_text(text)
    
    # ëª¨ë¸ ë¡œë“œ (ìºì‹œ ê²½ë¡œ í™•ì¸)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    vectorstore = Chroma.from_texts(
        texts=split_texts,
        embedding=embeddings,
        persist_directory=DB_PATH
    )
    vectorstore.persist()
    print("âœ… ì˜¤ëŠ˜ìž ë³´ê³ ì„œë¡œ Vector DB ì—…ë°ì´íŠ¸ ì™„ë£Œ")

# --- 3. AI Agent ì§ˆì˜ ì‘ë‹µ ---
def run_ai_agent(**kwargs):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # LLM ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ Endpoint IDë¡œ êµì²´ ê¶Œìž¥)
    # llm_ep = HuggingFaceEndpoint(
    #     repo_id="mistralai/Mistral-7B-Instruct-v0.2", 
    #     task="text-generation",
    #     max_new_tokens=512
    # )
    llm_ep = HuggingFaceEndpoint(repo_id="openai/gpt-oss-20b", task="conversational")
    llm = ChatHuggingFace(llm=llm_ep)
    
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, 
        retriever=retriever, 
        memory=memory
    )
    
    query = "ì˜¤ëŠ˜ìž ì•ˆì „ê´€ë¦¬ ì¼ì¼ìƒí™©ë³´ê³ ì„œì˜ 'ê¸°ìƒ í˜„í™©'ê³¼ 'ê¸°ìƒ ì „ë§' ë‚´ìš©ì„ ë¶ˆë › í˜•íƒœë¡œ ìš”ì•½í•´ì¤˜. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì¤˜."
    response = qa_chain({"question": query})
    
    print("\n" + "="*50)
    print(f"ðŸ¤– AI Agent ì‘ë‹µ:\n{response['answer']}")
    payload = {"text": (f"ðŸ“Œ *ì˜¤ëŠ˜ì˜ ì•ˆì „ê´€ë¦¬ìƒí™© ìš”ì•½*\n```{response['answer']}```")}

    requests.post(
        SLACK_WEBHOOK_URL,
        json=payload,
        timeout=10,
    )
    print("="*50)

# --- DAG ì •ì˜ ---
with DAG(
    'mois_report_slack_version2',
    default_args=default_args,
    schedule='0 7 * * *', # ë§¤ì¼ ì˜¤ì „ 7ì‹œ ì‹¤í–‰
    catchup=False
) as dag:

    t1 = PythonOperator(
        task_id='extract_report_task',
        python_callable=extract_report_text,
    )

    t2 = PythonOperator(
        task_id='build_vector_db_task',
        python_callable=build_vector_store,
    )

    t3 = PythonOperator(
        task_id='query_ai_agent_task',
        python_callable=run_ai_agent,
    )

    t1 >> t2 >> t3